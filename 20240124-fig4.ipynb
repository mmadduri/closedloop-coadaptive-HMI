{"cells":[{"cell_type":"markdown","metadata":{},"source":"## Figure 4 - affect of learning rates on user performance and encoder"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import wilcoxon as wilcoxon\n\n\n# meta analysis functions\nimport sys\nsys.path.append('/code/')\nfrom util import analysis\nfrom util import plotting\nfrom util import util_continuous as utils"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"PATH = '/data/'"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"with open(PATH + 'time-domain-error/time-domain-error-30sec-in-cm.pkl','rb') as handle:\n    td_error, td_error_first, td_error_last, t0_start, t0_end, t1_end, td_diff, td_diff_slow, td_diff_fast, td_diff_pos, td_diff_neg, td_diff_pD3, td_diff_pD4 = pickle.load(handle)\n\nwith open(PATH + 'encoder-estimation-data/encoder-decoder-data.pickle', 'rb') as handle:\n    encoder, encoder_r2, idx_dict, pos_vel_model, pos, dec_vels, decoders = pickle.load(handle)\nkeys = ['METACPHS_S106', 'METACPHS_S107','METACPHS_S108', 'METACPHS_S109', 'METACPHS_S110', 'METACPHS_S111', 'METACPHS_S112', 'METACPHS_S113', 'METACPHS_S114', 'METACPHS_S115', 'METACPHS_S116', 'METACPHS_S117', 'METACPHS_S118', 'METACPHS_S119']\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"Table of variables used \n\n| Variable | Shape | Shape Representations | Meaning |\n| --- | --- | --- | --- |\n| td_error | (2, 14, 8, 20770) | blocks x subjects x conditions x time points | this is the time domain error (Euclidean Distance) between the cursor and target at each time point\n\nExplanations of shapes:\n* number of blocks = 2 -- experimental setup\n* numebr of subject = 14 -- experimental setup\n* number of conditions = 8 -- experimental setup of decoder conditions"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"assert(td_error.shape == (utils.n_blocks, utils.n_keys, utils.n_conds, utils.min_time))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Import seaborn\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", rc=utils.sns_custom_params, font_scale=0.6)\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"label_size = 6\n## SETUP THE FIGURE HERE\n## HAVE TO RE-REUN FROM HERE TO \"CLEAR\" THE PLOT\nfig_error_rate = plt.figure(figsize = (6.3, 2), layout='constrained') # set the total figure size\nmosaic = \"\"\"\n    aabc\n    \"\"\"\n\n# set up the axes\nax_dict = fig_error_rate.subplot_mosaic(mosaic)\nfor ii in ax_dict:\n    plotting.remove_and_set_axes(ax_dict[ii], bottom=True, left=True)\n    ax_dict[ii].tick_params(axis='both', which='major', labelsize = label_size)\n    ax_dict[ii].tick_params(axis='both', which='minor', labelsize = label_size)\nfig_error_rate.patch.set_facecolor('white')\n\n# a\n# time-domain error \nax_dict['a'].set_ylabel(\"Mean Error (cm)\")\nax_dict['a'].set_xlabel(\"Time (minutes)\")\nax_dict['a'].set_title(\"Time-Domain Error - Learning Rates\")\n\n# b \nax_dict['b'].set_ylabel(\"% Change in Error\")\n\n# b \nax_dict['c'].set_ylabel(r'$|E_{f} - E_{i}|$')\n\n\nfig_error_rate.patch.set_facecolor('white')\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# axis a\nax = ax_dict['a']\n\ndata_fast = np.mean(td_error[:, :, utils.fast, utils.RAMP:], axis=(0, 2)) # (2, 7, 4, 20770)\ndata_slow = np.mean(td_error[:, :, utils.slow, utils.RAMP:], axis=(0, 2)) # (2, 7, 4, 20770)\n\nprint(\"N = \", data_fast.shape)\n\n# number of samples per seconds\ntrial_time = 300 # 300 second trials\ntscale = int(utils.min_time/trial_time) # number of samples per seconds\ntime_x = np.linspace(0, 5, utils.min_time) # time in minutes\nks = int(5*tscale) # set kernal size for smoothing to be in seconds\n\n# # check to make sure that we're comparing N of keys across the 2 conditions\naxis = 0\nassert(data_fast.shape[0] == utils.n_keys)\nassert(data_slow.shape[0] == utils.n_keys)\n\nplotting.plot_smooth_time_domain(time_x[utils.RAMP:], data_fast, utils.n_keys, ls='--', axis = axis, \n                                 kernal_size = ks, ax=ax, color=utils.colors['fast'], alpha=0.4,\n                                 lw=1.5, label='fast', remove_axes = False)\nplotting.plot_smooth_time_domain(time_x[utils.RAMP:], data_slow,  utils.n_keys, axis = axis, \n                                 kernal_size = ks, ax=ax, color=utils.colors['slow'],  alpha=0.4,\n                                 lw=1.5, label='slow', remove_axes = False)\n\n# ax.hlines(y=2.5, xmin = 0, xmax = 5, ls = '--', lw = 0.5, color  = 'black')\n\n# ax.set_ylim(0, 45)\nfig_error_rate"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"td_early = td_error[:, : ]"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# fig b - performance across learning rates\n\naxs = ax_dict['b']\n\ndata1 = np.ndarray.flatten(td_diff_slow)\ndata2 = np.ndarray.flatten(td_diff_fast)\n\n# data1 = np.median(td_error[:, :, utils.fast, :(70*20)], axis = (0, 2, 3))\n# data2 = np.median(td_error[:, :, utils.slow, :(70*20)], axis = (0, 2, 3))\nassert(data1.shape == data2.shape == (utils.n_keys,))\n\ndata_groups = [data1, data2] # slow, fast\ndata_labels = ['Slow', 'Fast']\ndata_pos = [0, 0.5]\nbplot = axs.boxplot(data_groups, \n                    showfliers=False,\n                    patch_artist=True,\n                    positions=data_pos,\n                    widths = 0.2,\n                    boxprops=dict(edgecolor=\"none\"),\n                    medianprops=dict(color='black', lw=1))\n\n\n\nfor patch, color in zip(bplot['boxes'], [utils.colors['slow'], utils.colors['fast']]):\n    patch.set_facecolor(color)\n\n# # rotate labels  \naxs.set_xticks(data_pos, data_labels, rotation=40)\n\nw = wilcoxon(data1, data2) \nprint(w)\n\nplotting.plot_significance(pvalue = w.pvalue, data1=data1, data2 = data2, \n                           data_pos = data_pos, fig=fig_error_rate, \n                           ax=axs, fontsize=10, lw=1, y_asterix=5, y_bar=3)\n\nfig_error_rate"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"## set up encoder changes\n\n## this code sets up the encoder variables to be plotted and compared in this notebook\n\n## set up variables to use for this code\n# number of features of the encoder\n# 8 features of the encoder = target position (x, y), target velocity (x, y), position error (x, y), velocity error (x, y)\nn_feat = 8 \n\n## take the encoder variables without the affine term \n# this is done because the affine term was to fit the encoder, but does not represent any encoding of the user\nencoder_linear = encoder[:, :, :, :, :, :-1]\n# check that encoder_linear variable is the right shape -- same as the encoder, but the last axis should be the number of features\nassert(encoder_linear.shape == (utils.n_blocks, utils.n_keys, utils.n_conds, \n                                len(utils.update_ix) - 2, utils.n_ch, n_feat))\n\n## average the encoder across minute to use the minute-averaged encoder as a measurement of the user\n# this averaging is done so that we can reduce the overfitting of the encoder to the 20-second intervals\naxis_time = 3 # the time axis is the 4th one, so = 3\nenc_all_min1 = np.mean(encoder_linear[:, :, :, 1:4], axis = axis_time) # average across the first minute\nenc_all_min2 = np.mean(encoder_linear[:, :, :, 4:7], axis = axis_time) # average across the 2nd minute\nenc_all_min3 = np.mean(encoder_linear[:, :, :, 7:10], axis = axis_time) # average across the 3rd minute\nenc_all_min4 = np.mean(encoder_linear[:, :, :, 10:13], axis = axis_time) # average across the 4th minute\nenc_all_min5 = np.mean(encoder_linear[:, :, :, -3:], axis = axis_time) # average across the last minute\nassert(enc_all_min1.shape == enc_all_min2.shape == enc_all_min3.shape \n       == enc_all_min4.shape == enc_all_min5.shape \n       == (utils.n_blocks, utils.n_keys, utils.n_conds, utils.n_ch, n_feat))\n\n# stack the minute-averaged encoders together so we can find the change in the users' encoders minute to minute\nenc_all_min = np.stack((enc_all_min1, enc_all_min2, enc_all_min3, enc_all_min4, enc_all_min5))\n# 5 in the first axis since we're stacking 5 arrays together\nassert(enc_all_min.shape == (5, utils.n_blocks, utils.n_keys, utils.n_conds, utils.n_ch,  n_feat))\n\n## subtract the difference in the encoder from minute to minute and then take the frobenious norm of that difference\n# this is |E_t+1 - E_t|_F\nenc_all_diff = np.linalg.norm(np.diff(enc_all_min, axis = 0), axis = (-1, -2))\n\n# this is |E_final - E_initial|_F\nenc_all_fi = np.linalg.norm(enc_all_min5 - enc_all_min1, axis = (-1, -2))\n# shape is subjects x trials \nassert(enc_all_fi.shape == (utils.n_blocks, utils.n_keys, utils.n_conds))\n\n# find the mean difference per subject\nenc_all_diff_subj = np.mean(enc_all_diff, axis = (0, 1, 3))\nenc_fi_diff_subj = np.mean(enc_all_fi, axis = (0, 2))\nassert(enc_all_diff_subj.shape == enc_fi_diff_subj.shape == (utils.n_keys, ))\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# get one |E_f - E_i\\ per condition per subject\n\n# SLOW, FAST - mean across subjects\nenc_fi_diff_fast = np.mean(enc_all_fi[:, :, utils.fast], axis = (0, 2))\nenc_fi_diff_slow = np.mean(enc_all_fi[:, :, utils.slow], axis = (0, 2))\n\n# POS, NEG - mean across subjects\nenc_fi_diff_pos = np.mean(enc_all_fi[:, :, utils.pos_init], axis = (0, 2))\nenc_fi_diff_neg = np.mean(enc_all_fi[:, :, utils.neg_init], axis = (0, 2))\n\n# LAMBDA HIGH VS LAMBDA LOW- median across subjects\nenc_fi_diff_pD4 = np.mean(enc_all_fi[:, :, utils.pD_3], axis = (0, 2))\nenc_fi_diff_pD3 = np.mean(enc_all_fi[:, :, utils.pD_4], axis = (0, 2))\n\nd_fast = np.ndarray.flatten(enc_fi_diff_fast)\nd_slow = np.ndarray.flatten(enc_fi_diff_slow)\nd_pos = np.ndarray.flatten(enc_fi_diff_pos)\nd_neg = np.ndarray.flatten(enc_fi_diff_neg)\nd_pd4 = np.ndarray.flatten(enc_fi_diff_pD4) # low penalty term\nd_pd3 = np.ndarray.flatten(enc_fi_diff_pD3) # high penalty term\n\n# check for shapes across comparison\nassert(len(d_fast) == len(keys))\nassert(len(d_slow) == len(keys))\nassert(len(d_pos) == len(keys))\nassert(len(d_neg) == len(keys))\nassert(len(d_pd4) == len(keys))\nassert(len(d_pd3) == len(keys))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"## add subfigure to the combined plots\n\naxs = ax_dict['c']\n\ndata_groups = [d_slow, d_fast]\ndata_labels = ['slow', 'fast']\ndata_pos = [0, 0.4]\nbplot = axs.boxplot(data_groups, \n                    showfliers=False,\n                    patch_artist=True,\n                    positions=data_pos,\n                    widths=0.2,\n                    boxprops=dict(edgecolor=\"none\"),\n                    medianprops=dict(color='k', lw=1))\n\n\nfor patch, color in zip(bplot['boxes'], \n                               [utils.colors['slow'], utils.colors['fast']]):\n            patch.set_facecolor(color)\n        \n# rotate labels  \naxs.set_xticks(data_pos,data_labels, rotation=40)\n\n# set labels and axes\n# axs.tick_params(axis='x', labelsize=tck_size)\n# learning rate\nw1 = wilcoxon(np.ndarray.flatten(d_slow), np.ndarray.flatten(d_fast))\nprint(\"learning rate, N = \", d_fast.shape)\npv1 = w1.pvalue\nprint(w1)\n\nw2 = wilcoxon(np.ndarray.flatten(d_pos), np.ndarray.flatten(d_neg))\nprint(\"init, N = \", d_pos.shape)\npv2 = w2.pvalue\nprint(w2)\n\nw3 = wilcoxon(np.ndarray.flatten(d_pd4), np.ndarray.flatten(d_pd3))\nprint(\"penalty, N = \", d_pd4.shape)\npv3 = w3.pvalue\nprint(w3)\n\n\n\naxs.set(ylabel = '$|E_{f} - E_{i}|$')\n\nplotting.plot_significance(pvalue = pv1, data1=d_slow, data2 = d_fast, \n                           data_pos = data_pos, fig=fig_error_rate, \n                           ax=axs, fontsize=10, lw=1, y_asterix=3.5, y_bar=3)\n\nfig_error_rate"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"image_format = 'pdf' # e.g .png, .svg, etc.\nimage_name = 'fig4-learning-rates-mean-cm.pdf'\nPATH = '/results/'\nfig_error_rate.savefig(PATH + image_name, format=image_format, dpi=300)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"analysis_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":2}
